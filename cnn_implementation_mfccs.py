# -*- coding: utf-8 -*-
"""CNN-IMPLEMENTATION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lz8LFjW3qzzM1AjADFvUf0bc_rMY-Zkn
"""

import json
import numpy as np
from sklearn.model_selection import train_test_split
import pandas as pd
import tensorflow.keras as keras
import sklearn.preprocessing as skp
import matplotlib.pyplot as plt
#split into training and testing
seed=12
np.random.seed(seed)

DATASET_PATH = "C:/ML-Final-Project/data.json"

def load_data(dataset_path):
    with open(dataset_path,"r") as fp:
        data = json.load(fp)

    #convert lists into numpy arrays
        inputs = np.array(data["mfcc"])
        targets = np.array(data["labels"])

        return inputs, targets

#load data
inputs,targets = load_data(DATASET_PATH)
inputs

inputs

inputs_train,inputs_test,targets_train,targets_test = train_test_split(inputs,targets,test_size=0.3) #30% for testing

#netwokr architecture
model = keras.Sequential([
    #input layer
    keras.layers.Flatten(input_shape=(inputs.shape[1],inputs.shape[2])),
    #1st hidden layer
    keras.layers.Dense(512,activation="relu"),
    #2nd hidden layer
    keras.layers.Dense(256,activation="relu"),

    #3rd hidden layer
    keras.layers.Dense(64,activation="relu"),

    #output layer
    keras.layers.Dense(10,activation="softmax")     #ACTIVATION FUNCTION TO NORMALIZE PREDICITIONS
])

#above model doens't account for overfitting

#compile network
optimizer = keras.optimizers.Adam(learning_rate=0.0001)
model.compile(optimizer=optimizer,loss="sparse_categorical_crossentropy",metrics =["accuracy"])
model.summary()

def plot_history(history):

    fig,axs = plt.subplots(2)

    #create accuracy subplots:
    # create accuracy sublpot
    axs[0].plot(history.history["accuracy"], label="train accuracy")
    axs[0].plot(history.history["val_accuracy"], label="test accuracy")
    axs[0].set_ylabel("Accuracy")
    axs[0].legend(loc="lower right")
    axs[0].set_title("Accuracy eval")

    # create error sublpot
    axs[1].plot(history.history["loss"], label="train error")
    axs[1].plot(history.history["val_loss"], label="test error")
    axs[1].set_ylabel("Error")
    axs[1].set_xlabel("Epoch")
    axs[1].legend(loc="upper right")
    axs[1].set_title("Error eval")

    plt.show()

def prepare_datasets(test_size, validation_size):
    """Loads data and splits it into train, validation and test sets.
    :param test_size (float): Value in [0, 1] indicating percentage of data set to allocate to test split
    :param validation_size (float): Value in [0, 1] indicating percentage of train set to allocate to validation split
    :return X_train (ndarray): Input training set
    :return X_validation (ndarray): Input validation set
    :return X_test (ndarray): Input test set
    :return y_train (ndarray): Target training set
    :return y_validation (ndarray): Target validation set
    :return y_test (ndarray): Target test set
    :return z : Mappings for data
    """

    # load data
    X, y, z = load_data(DATA_PATH)

    # create train, validation and test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)
    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)

    # add an axis to input sets
    X_train = X_train[..., np.newaxis]
    X_validation = X_validation[..., np.newaxis]
    X_test = X_test[..., np.newaxis]
    return X_train, X_validation, X_test, y_train, y_validation, y_test, z

#train the network
history = model.fit(inputs_train,targets_train,validation_data=(inputs_test,targets_test),epochs=50,batch_size=32)

#plot the accuracy and error over epochs

plot_history(history)

#ACCOUNT FOR OVERFITTING
model2 = keras.Sequential([
    #input layer
    keras.layers.Flatten(input_shape=(inputs.shape[1],inputs.shape[2])),

    #1st hidden layer
    keras.layers.Dense(512,activation="relu",kernel_regularizer=keras.regularizers.l2(0.001)),
    keras.layers.Dropout(0.3),
    #2nd hidden layer
    keras.layers.Dense(256,activation="relu",kernel_regularizer=keras.regularizers.l2(0.001)),
    keras.layers.Dropout(0.3),
    #3rd hidden layer
    keras.layers.Dense(64,activation="relu",kernel_regularizer=keras.regularizers.l2(0.001)),
    keras.layers.Dropout(0.3),
    #output layer
    keras.layers.Dense(10,activation="softmax")     #ACTIVATION FUNCTION TO NORMALIZE PREDICITIONS
])

#compile network
optimizer = keras.optimizers.Adam(learning_rate=0.0001)
model2.compile(optimizer=optimizer,loss="sparse_categorical_crossentropy",metrics =["accuracy"])
model2.summary()

history2 = model2.fit(inputs_train,targets_train,validation_data=(inputs_test,targets_test),epochs=50,batch_size=32)

plot_history(history2)

###BUILDING A CNN

def prepare_datasets(test_size,validation_size):
    X,y = load_data(DATASET_PATH)

    #split into train and test sets
    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=test_size)

    #create train/validation split
    X_train,X_validation,y_train,y_validation = train_test_split(X_train,y_train,test_size=validation_size)

    #3d-array expected  ->1 channel only
    X_train = X_train[...,np.newaxis]
    X_validation = X_validation[...,np.newaxis]
    X_test = X_test[...,np.newaxis]

    return X_train, X_validation, X_test, y_train,y_validation,y_test

X_train, X_validation, X_test, y_train,y_validation,y_test = prepare_datasets(0.25,0.2)
X_train.ndim

def build_model(input_shape):
    #create model
    model = keras.Sequential()

    #1st conv layer
    model.add(keras.layers.Conv2D(128,(2,2),activation='relu',input_shape=input_shape))
    model.add(keras.layers.MaxPool2D((3,3),strides=(2,2),padding='same'))
    model.add(keras.layers.BatchNormalization())

    #2nd conv layer
    model.add(keras.layers.Conv2D(64,(2,2),activation='relu',input_shape=input_shape))
    model.add(keras.layers.MaxPool2D((2,2),strides=(2,2),padding='same'))
    model.add(keras.layers.BatchNormalization())

    #3rd conv layer
    model.add(keras.layers.Conv2D(32,(2,2),activation='relu',input_shape=input_shape))
    model.add(keras.layers.MaxPool2D((2,2),strides=(2,2),padding='same'))
    model.add(keras.layers.BatchNormalization())

    #flatten output and feed to dense layer
    model.add(keras.layers.Flatten())
    model.add(keras.layers.Dense(64,activation='relu'))
    model.add(keras.layers.Dropout(0.3))

    #output layer
    model.add(keras.layers.Dense(10,activation='softmax'))

    return model

#Building the CNN
input_shape = (X_train.shape[1],X_train.shape[2],X_train.shape[3]) #X_train is a 4D array
model3= build_model(input_shape)

#compile the network
optimizer = keras.optimizers.Adam(learning_rate=0.0001)
model3.compile(optimizer=optimizer,loss="sparse_categorical_crossentropy",metrics =["accuracy"])

#train the model
model3.fit(X_train,y_train,validation_data=(X_validation,y_validation),batch_size=32,epochs=30)


#evaluate CNN on test ste
test_error,test_accuracy = model.evaluate(X_test,y_test,verbose=1)
print("Accuracy on test set is:{}".format(test_accuracy))

#evaluate CNN on test ste
test_error,test_accuracy = model3.evaluate(X_test,y_test,verbose=1)
print("Accuracy on test set is:{}".format(test_accuracy))

def predict(model,X,y):
    X = X[np.newaxis, ...]
    prediction = model.predict(X)  #X is a 3D array and predicition is a 2d array

    #extract index with max value
    predicted_index = np.argmax(prediction,axis=1)
    print("The expected index:{}, Predicted index:{}".format(y,predicted_index))

#make prediciton on a smaple
X =X_test[70]
y = y_test[70]
predict(model3,X,y)

len(X_test)
y_pred = []
for i in range(len(X_test)):
    Y = X_test[i]
    Y = Y[np.newaxis,...]
    prediction = model3.predict(Y)
    y_pred.append(np.argmax(prediction,axis=1))
len(y_pred)

from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, roc_auc_score, roc_curve, auc
from scipy import interp
import itertools
from itertools import cycle
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

    plt.savefig('CNN-Confusion-Matrix.jpg')

def one_hot_encoder(true_labels, num_records, num_classes):
    temp = np.array(true_labels[:num_records])
    true_labels = np.zeros((num_records, num_classes))
    true_labels[np.arange(num_records), temp] = 1
    return true_labels

label_dict ={'blues':0,'classical':1,'country':2,'disco':3,'hiphop':4,'jazz':5,'metal':6,'pop':7,'reggae':8,'rock':9}
label_dict.keys()

plot_confusion_matrix(confusion_matrix(y_true=y_test, y_pred=y_pred),
                      classes=label_dict.keys())

"""##"""

